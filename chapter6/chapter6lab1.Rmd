---
title: "Model Selection - Best Subset"
author: "Steve Moore"
date: "January 1, 2016"
output: html_document
---

Model Selection
===============

```{r}
library(ISLR)
summary(Hitters)
# remove cases with missing values
Hitters = na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
```

## Best Subset regression

By default, up to eight variables.

```{r}
library(leaps)
regfit.full = regsubsets(Salary ~ .,
                         data = Hitters)
summary(regfit.full)
```

All variables.

```{r}
regfit.full = regsubsets(Salary ~ .,
                         data = Hitters,
                         nvmax=19)
summary(regfit.full)
reg.summary = summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,
     xlab="Number of variables",
     ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
```

There is a plot method for the regsubsets object.

```{r}
plot(regfit.full, scale="Cp")
coef(regfit.full,10)
```

## Forward Stepwise Selection

```{r}
regfit.fwd=regsubsets(Salary ~ .,
                      data = Hitters,
                      nvmax=19,
                      method="forward")
reg.summary = summary(regfit.fwd)
which.min(reg.summary$cp)
plot(regfit.fwd,
     scale="Cp")
plot(reg.summary$cp,
     xlab="Number of variables",
     ylab="Cp")
points(10, reg.summary$cp[10],
       pch=20,
       col="red")
coef(regfit.fwd,10)
```

## Model Selection Using a Validation Set

```{r}
dim(Hitters)
set.seed(1)
train = sample(seq(263),180,replace=FALSE)
train
reg.fwd = regsubsets(Salary ~ .,
                     data = Hitters[train,],
                     nvmax=19,
                     method="forward")
```

Predict using test set.  NO predict method for regsubsets,
so we have to hack a bit.

```{r}
val.errors = rep(NA,19)
x.test = model.matrix(Salary ~ .,
                      data = Hitters[-train,])
for(i in 1:19) {
  # coefi = coef(regfit.fwd,id=i)
  coefi = coef(reg.fwd,id=i)
  # element-wise multiplication by coefficients
  pred = x.test[,names(coefi)] %*% coefi
  val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
sqrt(val.errors)
plot(sqrt(val.errors),
     ylab="Root MSE",
     ylim=c(250,400),
     pch=19,
     type="b")
points(sqrt(reg.fwd$rss[-1]/180),
       col="blue",
       pch=19,
       type="b")
legend("topright",
       legend=c("Training","Validation"),
       col=c("blue","black"),
       pch=19)

predict.regsubsets = function(object,newdata,id,...) {
  form = as.formula(object$call[[2]])
  mat=model.matrix(form, newdata)
  coefi=coef(object,id=id)
  mat[,names(coefi)] %*% coefi
}
```

## Model Selection By Cross-Validation

```{r}
set.seed(1)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors = matrix(NA,10,19)
for (k in 1:10) {
  best.fit = regsubsets(Salary ~ .,
                        data = Hitters[folds != k,],
                        nvmax=19,
                        method="forward")
  for (i in 1:19) {
    pred = predict(best.fit,Hitters[folds==k,],id=i)
    cv.errors[k,i] = mean( (Hitters$Salary[folds==k] - pred)^2)
  }
}
rmse.cv = sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,
     pch=19,
     type="b")
```

### Ridge Regression and Lasso

```{r}
library(glmnet)
x = model.matrix(Salary ~ ., 
                 data = Hitters)
y = Hitters$Salary
```

### Ridge regression model.

```{r}
# alpha = 0 is ridge; alpha = 1 is lasso; 
# between 0 and 1 you get elastic net models
fit.ridge = glmnet(x, y, alpha = 0)
plot(fit.ridge, 
     xvar = "lambda",
     label = TRUE)
# 10-fold cross-validation by default
cv.ridge = cv.glmnet(x, y, alpha = 0)
plot(cv.ridge)
```

### Lasso Model

```{r}
fit.lasso = glmnet(x,y)
plot(fit.lasso,
     xvar = "lambda",
     label = TRUE)
plot(fit.lasso,
     xvar = "dev",
     label = TRUE)
cv.lasso = cv.glmnet(x,y)
plot(cv.lasso)
coef(cv.lasso)
```

Train / validation division to select teh "lambda" for the lasso.
 
```{r}
lasso.tr = glmnet(x[train,],y[train])
lasso.tr
pred = predict(lasso.tr, x[-train,])
dim(pred)
rmse = sqrt(apply((y[-train] - pred)^2,2,mean))
plot(log(lasso.tr$lambda), rmse, type="b", xlab="Log(lambda)")
lam.best = lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr,s=lam.best)
```





